\chapter{Proposed Method}\label{chap:proposed_method}
\section{Overview}\label{sec:overview}
In this thesis I present a method to increase the efficiency of finding unique columns in a table. The method is based on a machine learning model which uses the first few rows of each column to guess if it will have any duplicate values. Each positive guess will subsequently be validated using a conventional naive method.

The proposed method works in three steps. First, the features are extracted from the first rows of the table. After that, the model tries to predict the existence of duplicate values from the features. Finally, the columns which are unique according to the model are checked with a naive method.

The source code of the proposed method as well as the experiments in Chapter~\ref{chap:experiments} can be found in the GitHub repository \url{https://github.com/LUH-DBS/Prange}.

% ["Duplicates", "Data Type", "Sorted",
%           # number
%           "Min. value", "Max. value", "Mean", "Std. Deviation",
%           # string
%           "Avg. string length", "Min. string length", "Max. string length"
%           ]  # 10
\section{Feature Extraction}\label{sec:extracted_features}
The machine learning model which is used by the proposed method cannot work on the tables directly as it is trained with supervised learning. The model therefore uses a feature table as input, an example of which can be seen in Table~\ref{table:feature_table_example}.

\input{table-code/proposed-method/feature_table.tex}

The code in Listing~\ref{lst:proposed_method-prepare_column} extracts the features from each column to produce such a feature table. The feature extraction of the proposed method works in different steps which are executed one after the other.

The variable column contains the first \textit{n} rows of the column in the table where \textit{n} is the input size of the method.

First all columns which contain duplicate values in the first rows are sorted out in line 1 and 2 by setting the feature \enquote{Duplicates} to \num{1}, all other features to \num{0} and returning the row.

In line 5 to 12 the remaining columns are checked in order to determine whether they are sorted or not. During this step, it is possible that an error occurs because two values in the column cannot be compared. This is mostly the case if there is an empty value in the column. In this case the column is considered not sorted.

In the following lines, a distinction is made between the different types of values.

If the column contains only boolean values, the feature \enquote{Data Type} is set to \num{3} while all other features stay on \num{0} in line 13 to 16. Although it is very unlikely that a column contains exclusively boolean values without any duplicate values, there are tables in the GitTables dataset that is used during the experiments in Chapter~\ref{chap:experiments} to which this applies.

Line 17 to 23 handle columns that contain exclusively numeric values. The \enquote{Data Type} feature is set to \num{1} in line 18 and the four features specific to numeric values are extracted in line 19 and 20. The three remaining features are set to \num{0} in line 22.

Finally, the columns that contain exclusively string or a mix of different types are handled in the remaining lines.

First, the features for numeric values are set to \num{0} in line 25. After that, the average of the length of every item in the column is formed together with the minimum and maximum length in line 35 to 40. At the end the \enquote{Data Type} feature is set to \num{2} in line 41.

If there is any value in the column which is not a string, the \enquote{Data Type} feature is set to \num{4} in line 32 and the string specific features set to \num{0}.

It should be emphasized that the column header is not being extracted as a feature. While this could lead to a better performance in some cases, it would be challenging to encode the various headings in a way that is understandable for the machine learning model.

\lstinputlisting[
  float,
  numbers=left,
  language=Python,
  caption={[The algorithm to extract the features from a column]This code shows how the features are extracted from a column. This process is repeated for each column; the result forms the feature table which is the input for the machine learning model. The variable \texttt{column} contains the first \textit{n} rows of the column where \textit{n} is the input size of the model.},
  label={lst:proposed_method-prepare_column}
]{table-code/proposed-method/prepare-columns.py}


\section{Used packages and libraries}\label{sec:used_packages_and_libraries}
For this thesis, the algorithms and experiments where implemented using Python 3.10.2. The model was trained with Auto-Sklearn, a python library that implements automated machine learning on top of the popular library Scikit-learn. To interact with the data, the python library pandas was used as its dataframe structure has useful additional functions as well as a good compatibility to Scikit-learn.


\section{Training the Model}\label{sec:traing_the_model}
The machine learning model was trained on the training dataset, which is a subset with \num{10000} tables of the GitTables dataset~\cite{gittables-article} that is used for the correctness test in Section~\ref{sec:correctness}. Each of the tables has a minimum size of \num{100} rows and \num{3} columns.

The features where extracted from each table in the training dataset and saved as the \enquote{training\_data} table. Additionally, the data was correctly classified using the naive algorithm to form the \enquote{training\_result} table.

The Listing~\ref{lst:proposed_method-train_model} presents the training process. First in line 5 and 6 the feature table and the result table where loaded as pandas dataframes.

In line 8 to 11, the training parameters where selected. The train time was changed for the different experiments in Chapter~\ref{chap:experiments}. The model was always trained with the metric recall used to measure its performance, as false negatives have the highest cost for the performance of the model. While there is a cost associated with false positives too, they do not decrease the correctness of the proposed method as each positive guess by the model is verified using a naive algorithm.

Finally, in line 12 the model is trained on the training data. Auto-Sklearn automatically chooses the best of a range of classical learning algorithms.

\lstinputlisting[
  float=ht,
  numbers=left,
  language=Python,
  caption={[Training the model with Auto-Sklearn]This code shows how the machine learning model is trained using the python library Auto-Sklearn.},
  label={lst:proposed_method-train_model}
]{table-code/proposed-method/train_model.py}
