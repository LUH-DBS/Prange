\chapter{Proposed Method}\label{chap:proposed_method}
\section{Overview}\label{sec:overview}
In this thesis I present a method to increase the efficiency of finding unique columns in a table. The method is based on a machine learning model which uses the first few rows of each column to guess if it will have any duplicate values. Each positive guess will subsequently be validated using a conventional naive method.

The proposed method works in three steps. First, the features are extracted from the first rows of the table. After that, the model tries to predict the existence of duplicate values from the features. Finally, the columns which are unique according to the model are checked with a naive method.

The source of the proposed method as well as the experiments in Chapter~\ref{chap:experiments} can be found in the GitHub repository \url{https://github.com/LUH-DBS/Prange}. % TODO: write better?

% ["Duplicates", "Data Type", "Sorted",
%           # number
%           "Min. value", "Max. value", "Mean", "Std. Deviation",
%           # string
%           "Avg. string length", "Min. string length", "Max. string length"
%           ]  # 10
\section{Feature Extraction}\label{sec:extracted_features}
The machine learning model which is used by the proposed method can not work on the tables directly as it is trained with supervised learning. The model therefore uses a feature table as input, an example of which can be seen in Table~\ref{table:feature_table_example}.

\input{table-code/proposed-method/feature_table.tex}

% TODO: which data type numbers exist? -> is inside the text sufficient?

The code in Listing~\ref{lst:proposed_method-prepare_column} extracts the features from each column to produce such a feature table. The feature extraction of the proposed method works in different steps which are executed one after the other.

The variable column contains the first \textit{n} rows of the column in the table where \textit{n} is the input size of the method.

First all columns which contain duplicate values in the first rows are sorted out in line 1 and 2 by setting the feature \enquote{Duplicates} to \num{1} and all other features to \num{0}.

In line 5 to 12 the remaining columns are checked in order to determine whether they are sorted or not. During this step, it is possible that an error occurs because two values in the column can not be compared. This is mostly the case if there is an empty/None value in the column.

In the following lines, a distinction is made between the different types of values.

If the column contains only boolean values, the feature \enquote{Data Type} is set to \num{3} while all other features stay on \num{0} in line 13 to 16. Although it is very unlikely that a column contains exclusively boolean values without any duplicate values, there are tables in the GitTables dataset to which this applies.

Line 17 to 23 handle columns that contain exclusively numeric values. The \enquote{Data Type} feature is set to \num{1} in line 18 and the four features specific to numeric values are extracted in line 19 and 20. The three remaining features are set to \num{0} in line 22.

Finally, the columns that contain exclusively string or a mix of different types are handled in the remaining lines.

First, the features for numeric values are set to 0 in line 25. After that, the average of the length of every item in the column is formed together with the minimum and maximum length. At the end the \enquote{Data Type} feature is set to \num{2} in line 39.

If there is any value in the column which is not a string, a ValueError is raised in line 32, which leads to the \enquote{Data Type} feature being set to \num{4} in line 42 and the string specific features set to \num{0}.

It should be emphasized that the column header is not being extracted as a feature. While this could lead to a better performance in some cases, it is challenging to encode the various headings in a way that is understandable for the machine learning model.

\lstinputlisting[
  float,
  numbers=left,
  language=Python,
  caption={[The algorithm to extract the features from a column]This code shows how the features are extracted from a column. This process is repeated for each column; the result forms the feature table which is the input for the machine learning model. The variable \texttt{column} contains the first \textit{n} rows of the column where \textit{n} is the input size of the model.},
  label={lst:proposed_method-prepare_column}
]{table-code/proposed-method/prepare-columns.py}


\section{Used packages and libraries}\label{sec:used_packages_and_libraries} % TODO: maybe another paragraph?
For this thesis, the algorithms and experiments where implemented using Python 3.10.2. To interact with the data, the python library pandas was used as its dataframe structure has useful additional functions and has a good compatibility to the library Auto-Sklearn, which was used to train the machine learning model.


\section{Training the Model}\label{sec:traing_the_model}
The machine learning model was trained on the training dataset, which is a subset with \num{10000} tables of the GitTables dataset that is used for the correctness test in Section~\ref{sec:correctness}. Each of the tables has a minimum size of \num{100} rows and \num{3} columns. % TODO: write better, explain GitTables further?

From each table in the training dataset the features where extracted and saved as the \enquote{training\_data} table. Additionally, the data was correctly classified using the naive algorithm to form the \enquote{training\_result} table.

The Listing~\ref{lst:proposed_method-train_model} presents the training process. First, in line 5 and 6 the feature table produced from the training data and the result table where loaded as pandas dataframes.

In line 8 to 11, the training parameters where selected. The train time was changed for the different experiments in Chapter~\ref{chap:experiments}. The model was always trained with the metric recall used to measure its performance, as false negatives have the highest cost for the performance of the model. While there is a cost associated with false positives too, they do not decrease the correctness of the proposed method as each positive guess by the model is verified using a naive algorithm.

Finally, in line 12 the model is trained on the training data. Auto-Sklearn automatically chooses the best of a range of classical learning algorithms. % TODO: write better? 

\lstinputlisting[
  float=ht,
  numbers=left,
  language=Python,
  caption={[Training the model with Auto-Sklearn]This code shows how the machine learning model is trained using the python library Auto-Sklearn.}, % TODO: The data is loaded as pandas dataframes?
  label={lst:proposed_method-train_model}
]{table-code/proposed-method/train_model.py}
