\section{Metrics}\label{sec:metrics}
% https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9
During the training and testing of a machine learning model, its performance is measured with different metrics which indicate how closely the prediction of the machine learning model matches reality. Table~\ref{table:true-false-neg-pos} shows the different possible relations between prediction and reality.

\begin{table}[ht]
  \centering
  \caption{Definition of true and false negatives and positives}
  \begin{tabular}{ll|l|l} % chktex 44
                            & \multicolumn{1}{c}{} & \multicolumn{2}{c}{Prediction}                  \\
                            &                      & Negative                       & Positive       \\ \cline{2-4}
    \multirow{2}{*}{Actual} & Negative             & True Negative                  & False Positive \\ \cline{2-4}
                            & Positive             & False Negative                 & True Positive  \\
  \end{tabular}\label{table:true-false-neg-pos}
\end{table}

Four metrics exist based on those measurements to calculate the correctness of the machine learning model.

The first of these metrics is accuracy, which is calculated by dividing the number of correct guesses by the total number of guesses. It indicates how close the prediction is to the actual result and is therefore a good general reference point for the correctness of the model.

The second metric, precision, is defined as the number of true positive guesses divided by the total number of positive guesses. It is very well suited for determining the correctness when false positives are associated with a high cost.

Recall is the third metric and defined as the number of positive guesses divided by the number of actual positives. It is a good metric to measure the performance when false negatives are associated with a high cost.

Finally, F1 is calculated by dividing the product of precision and recall by their sum. Just like accuracy, F1 is well suited when both precision and recall are equally important. The difference is that F1 is especially good in cases where the number of positives and negatives is very unbalanced. An example for this is a dataset where \SI{99}{\percent} of the actual results are negatives.
