\section{Efficiency}\label{sec:efficiency}
Another important metric to determine the feasibility of the machine learning model is the efficiency. The main question is, if or from what table size the model is faster than a naive method. This becomes even more interesting as each positive guess of the model has to be verified using the naive algorithm because the model is trained for the highest recall and precision, not accuracy. % TODO: ref to model


\subsection{Experiment Data}\label{subsec:efficiency-experiment_data}
The experiment was conducted on a set of generated tables to control for the size of the table as well as the number of unique and non-unique columns, as can be seen in Table~\ref{table:efficiency-generated_table}.

For the experiments, tables with different sizes were generated. They each had \num{10} columns and between \num{100} and \num{100000000} columns. To ensure the model the correct prediction by the model, the columns where generated in a specific way. The unique columns are evenly incrementing for the first \num{50} rows, while the first two rows of the non-unique columns contain the same value. The rest of each column contains distinct incrementing values which are mixed up to increase the time of the naive algorithm which works by sorting each column.

\input{appendix/experiments/efficiency-generated_table.tex}


\subsection{Base experiment}\label{subsec:efficiency-base_experiment}
For the first experiment, a model which uses 10 rows as its input was used. The experiment data consisted of \num{9} tables with \num{10} columns each and between \num{100} and \num{100000000} rows. Three of the columns where unique, seven contained duplicates.

Table !!! shows that for tables with up to \num{100000} rows, the naive algorithm takes only a fraction of a second and is therefore faster than the proposed machine learning model. However, since the model takes a more or less constant time of half a second, it becomes faster as the tables size surpasses one million rows.

Particularly noteworthy is the fact that the validation time for the model takes very close to \SI{30}{\percent} of the time of the naive algorithm. That is because only columns which are unique according to the model are validated, so in this case three.

In conclusion, it is clear that the most time is used for loading the dataset and checking the columns for duplicates with the naive algorithm. While a possible improvement of the naive algorithm is not part of this thesis, in Section~\ref{subsec:efficiency-shorter_loading_times} two different ways to shorten the loading times will be explored.


\subsection{Shortening loading times}\label{subsec:efficiency-shorter_loading_times}
While CSV files are very easy to use, they are not meant to efficiently store large data. Parquet files are a substantially more suitable file format to handle large data. As an additional benefit, this file format makes it possible to read only some columns because the data is saved column wise in contrast to CSV files. % TODO: is a citation necessary? (https://parquet.apache.org/)
Through the use of row groups it is possible to read only some rows too.

This property enables the model to first read only the first rows before the model makes its prediction and in a second step read only the columns which need to be validated.



\subsection{Comparing models with different input sizes}\label{subsec:efficiency-comparing_models}


\subsection{Changing the ratio of unique columns}\label{subsec:efficiency-changing_uniques}


\subsection{Summarized results}\label{subsec:efficiency-summarized_results}
