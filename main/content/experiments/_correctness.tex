\section{Correctness}\label{sec:correctness}
The correctness of the model is probably the most important metric to determine its usability. While a false positive is not a major problem for the correctness because each positive guess is verified (see Chapter~\ref{chap:proposed_method}), a false negative leads to a primary key candidate being ignored.

In this section, different experiments will be conducted to determine which parameters have an influence on the correctness of the prediction. Additionally, in Section~\ref{subsec:correctness_examine-false-guesses} the columns which led to false guesses by the model will be examined.


\subsection{Experiment Data}\label{subsec:correctness_experiment-data}
The experiments where performed on the GitTables dataset, which is a large corpus of relational tables extracted from CSV files in GitHub~\cite{gittables-article}. However, not the whole dataset was used, only a subset of tables which were split into a training and a test dataset.

To train each of the models tested in the following experiments, including the efficiency experiments, the training dataset was used. It is a subset of the GitTables dataset with \num{10000} tables that where selected by traversing the GitTables dataset and skipping tables which are too small.

Every table in the training dataset has to have at least \num{100} rows and \num{3} columns to ensure the feature extraction described in Section~\ref{lst:proposed_method-prepare_column} does not become the naive algorithm with extra steps. This could be a problem as searching for duplicates with the naive algorithm in the first \textit{n} rows of each column is part of the feature extraction.

The test dataset was generated the same way as the training dataset. By traversing the GitTables and skipping every table which is too small or part of the training dataset, a collection of \num{5000} tables with \num{57211} columns and an average of \num{184} rows was generated. It was used for every experiment apart from the one in Section~\ref{subsec:correctness_comparing-input-size} where a dataset of \num{30000} tables with \num{307030} columns and an average of \num{277} rows was used.


\subsection{Comparing models with different input sizes}\label{subsec:correctness_comparing-input-size}
As described in Section~\ref{sec:extracted_features}, the proposed method extracts features from the first rows of a table and uses a machine learning model to predict if a column has any duplicate values based on those features.

In this experiment, I compare different models which use the first \num{5}, \num{10}, \num{20} and \num{50} rows of each column to extract features. A model with an input size larger than \num{50} rows was not feasible as the tables used for training and testing had a minimum size of \num{100} rows.
% With a bigger input, the feature extraction would end up performing like the naive algorithm as checking the first \textit{n} rows for duplicates with the naive algorithm is part of the feature extraction. % TODO: is the last sentence too much? practically the same is said in the previous subsection


Each model was trained for \num{5} hours on the training dataset. During the experiment, the test dataset with \num{5000} tables that was described in Section~\ref{subsec:correctness_experiment-data} was used to test each model.

Table~\ref{table:correctness-comparing_input_sizes} shows the results of this experiment, which demonstrates that the input size has a large influence on the quality of the prediction. While none of the models have any false negative guesses, the number of false positive guesses decreases with an increasing input size.

%! TODO: how many positive guesses? -> guess_count.csv -> even for the 5 row model 40% of the columns predicted as positive

This experiment shows that an increase in the input size of the model does have a great impact on the number of false positive guesses. However, since each positive guess by the model is verified using the naive algorithm, the number of false guesses has no effect on the quality of the final prediction of the proposed method. What is however effected by it is the efficiency of the method; this is explored further in Section~\ref{subsec:efficiency-changing_uniques}.

Another important finding of this experiment is that even with a small input size of only \num{5} rows, the model has not made any false negative guesses. As the negative guesses of the model are not checked, it is important for them to be correct to ensure the overall correctness of the proposed method.

\input{table-code/result/correctness/compare_input_sizes.tex}


\subsection{Altering the training time}\label{subsec:correctness_comparing-training-time}
The Auto-Sklearn library, which was used to train the machine learning models, automatically searches for the best learning algorithm and optimizes it as described in Section~\ref{sec:used_packages_and_libraries}. Since this process takes time to run, in theory the performance of the model increases with higher training time. % TODO: maybe write better? 

In this experiment, different models with an input size of \num{10} rows are being trained for different amounts of time on the training dataset. Subsequently, the experiment is conducted for each model on the test dataset with \num{5000} tables.

The Table~\ref{table:correctness-compare_training_time} presents the results of this experiment. With a training time of one minute the performance of the machine learning model is indeed slightly worse as there are some false negative guesses.

However, apart from the false negative guesses, the models with a training time of one and two minutes have a slightly better performance than the other models. % TODO: maybe write better?

This experiment very clearly demonstrates that the machine learning model does not need a long training time to find unique columns. Furthermore, it shows that there might be advantageous to train a model multiple times and compare them to find the best performing, since a longer training time does not translate to higher performance.

\input{table-code/result/correctness/compare-training-time.tex}


\subsection{Summary}\label{subsec:correctness_conclusions}
The experiments in this section demonstrate that the predictions made by the model have a sufficiently high accuracy. Models which have been trained for more than one minute had a recall of 1 and therefore no false negative guesses. This is a very good result as false negative guesses would lead to unique columns which are ignored.

Furthermore, the models in the experiments produce less than twice as much false positive guesses as true positive guesses and even less for models with a larger input size. This is important as false positive guesses reduce the efficiency, which is explored further in Section~\ref{sec:efficiency}.

% TODO: the number of predicted positive correlates roughly with the number of actual positives


\subsection{Examine columns which led to false guesses}\label{subsec:correctness_examine-false-guesses} % TODO: maybe a table with examples?
The greatest weakness of the proposed method are false guesses. False positive guesses lead to a reduced efficiency because more columns need to be checked with the naive algorithm. False negative guesses on the other hand result in primary key candidates being ignored which reduces the correctness. It is therefore important to examine the columns which lead to false guesses to improve the model if possible.

False positive guesses occur very often as the model is primarily trained to avoid any false negative guesses. The experiment in Section~\ref{subsec:correctness_comparing-input-size} has shown that depending on the input size between \SI{27}{\percent} and \SI{78}{\percent} of the positive guesses are false positives.

The false guesses are unfortunately mostly unavoidable as they are caused mainly by empty cells which are located after the input rows of the model in otherwise unique columns. As the column would be a primary key candidate without these missing values, there is no possible change which would improve the correctness of the model in this case.

Another example for a column leading to a false positive guess is one containing the name of authors. Since the model only sees short strings without duplicates in these columns, there is no good way for the current implementation to recognize the column as non-unique. However, it would be possible to additionally include the column heading as a feature to enable the model to learn that the column with the names of authors is more likely to contain duplicate values.

False negative guesses occurred only in one of all the correctness experiments, namely with the model which was trained for just one minute in Section~\ref{subsec:correctness_comparing-training-time}. And even then the false negatives were rare, and the corresponding columns contained only negative numbers or numbers smaller than 1. While these columns are negative, they are not suited very well as primary keys. % TODO: is this ok?

% TODO: Maybe a summarizing paragraph?
