\section{Correctness}\label{sec:correctness}
The correctness of the model is probably the most important metric to determine its usability. While a false positive is not a major problem because each positive guess is verified (see Chapter~\ref{chap:proposed_method}), a false negative will mean that a primary key candidate gets ignored.

In this section, different experiments will be conducted to determine which parameters are the best to train the model. Additionally, in Section~\ref{subsec:correctness_examine-false-guesses} the column which led to false guesses by the model will be examined.


\subsection{Experiment Data}\label{subsec:correctness_experiment-data} % TODO: Writing
The experiments where performed on the gittables dataset, which is a large corpus of relational tables extracted from CSV files in GitHub\cite{gittables-article}.

For these experiments, only the tables with at least 100 rows and 3 columns where used. % TODO: how many exist in the dataset?

How was the training and the test dataset generated? % TODO


\subsection{Comparing models with different input sizes}\label{subsec:correctness_comparing-input-size}
% TODO: is the first paragraph too much before the experiment?
As described in Section~\ref{sec:extracted_features}, the proposed method extracts features from the first rows of a table and uses a machine learning model to predict if a column has duplicate values based on those features. % TODO: is ref to extracted features correct?

In this experiment, we compare different models which use \num{5}, \num{10}, \num{20} and \num{50} rows to extract features. A model with an input size larger than \num{50} rows was not feasible as the tables used for training and testing had a minimum size of \num{100} rows. With a bigger input, the preparation would end up working like the naive algorithm with an extra step.


Each model was trained for \num{5} hours. During the experiment, the test dataset with \num{5000} tables that was described in Section~\ref{subsec:correctness_experiment-data} was used to test each model. % TODO: is the test dataset described good enough?

Table~\ref{table:correctness-comparing_input_sizes} shows the results of this experiment, which demonstrate that the input size has a large influence on the quality of the prediction. While none of the models has any false negative guesses, the ratio of false positive guesses decreases with an increasing input size.

This experiment shows that an increase in the input size of the model does have a great impact on the number of false positive guesses. However, since each positive guess by the model is verified using the naive algorithm, the number of false guesses has no effect on the quality of the final prediction of the proposed method. What is effected by it is the efficiency of the method; this is explored further in Section~\ref{subsec:efficiency-changing_uniques}.

Another important finding of this experiment is that even with a small input size of only \num{5} rows, the model has not made any false negative guesses. As the negative guesses of the model are not checked, it is important for them to be correct to ensure the overall correctness of the proposed method.

\input{table-code/result/correctness/compare_input_sizes.tex}


\subsection{Altering the training time}\label{subsec:correctness_comparing-training-time} % TODO: Writing
The Auto-Sklearn library, which was used to train the machine learning models, automatically searches for the best learning algorithm and optimizes it, as was described in Section~\ref{sec:used_packages_and_libraries}. Since this process takes time to run, the theory is that the performance of the model increases with higher training time. % TODO: maybe write better? 

In this experiment, different models with an input size of \num{10} rows are being trained for different amounts of time. Each model is trained on \num{10000} tables with at least \num{100} rows and \num{3} columns. Subsequently, each of the models is tested on \num{5000} different tables.

The Table~\ref{table:correctness-compare_training_time} presents the results of this experiment. With a training time of \num{1} or \SI{2}{\minute} the performance of the machine learning model is indeed slightly worse as there are some false positive guesses. %! How many?

However, apart from producing false negative guesses, the models with the two shortest training times seem to performed slightly better than most of the other models as their accuracy and precision is higher. Additionally, the model with the best performance is not the one with the highest training time but the one a training time of \SI{120}{\minute}.

This experiment very clearly demonstrates that the machine learning model for this task does not need a long training time. Furthermore, it shows that there might be advantageous to train a model multiple times and compare them to find the best performing, since the performance is not strongly correlated to the training time.

\input{table-code/result/correctness/compare-training-time.tex}

% \subsection{Testing only non-trivial columns}\label{subsec:correctness_non-trivial-columns} % TODO: Writing
% Experiment Setup

% Result

% Conclusion

\subsection{Summary}\label{subsec:correctness_conclusions} % TODO: Writing


\subsection{Examine columns which led to false guesses}\label{subsec:correctness_examine-false-guesses} % TODO: Writing %? maybe a table with examples?
The greatest possible weakness of the of the proposed method are false guesses, as false positive guesses lead to a reduced efficiency and false negative guesses result in primary key candidates being ignored. It is therefore important to examine the columns which lead to false guesses to improve the model if possible.

False positive guesses occur very often as the model is trained to avoid any false negative guesses. The experiment in Section\ref{subsec:correctness_comparing-input-size} has shown that depending on the input size between \SI{18}{\percent} and \SI{77}{\percent} of the positive guesses are false positives. % TODO: how many guesses in total are correct/false
% The Section~\ref{subsec:efficiency-changing_uniques} will explore further how strong false positives influence the efficiency.

The false guesses are unfortunately mostly unavoidable as they are caused by empty cells which are located after the input rows of the model. As the column would be a primary key candidate without these missing values, there is nothing that can be changed to improve the correctness of the model in this case.

Another example for a column leading to a false positive guess is one containing the name of authors. For the model, this column contains short strings which do not have any duplicates in the first rows. % TODO: write further

False negatives

% Maybe a summarizing paragraph
