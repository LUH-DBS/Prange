% LTex: language=en-US enabled=false

@inbook{parquet-book,
  keywords  = {dataformat},
  title     = {Apache Parquet},
  author    = {Vohra, Deepak},
  year      = {2016},
  url       = {https://doi.org/10.1007/978-1-4842-2199-0_8},
  abstract  = {Apache Parquet is an efficient, structured, column-oriented (also called columnar storage), compressed, binary file format. Parquet supports several compression codecs, including Snappy, GZIP, deflate, and BZIP2. Snappy is the default. Structured file formats such as RCFile, Avro, SequenceFile, and Parquet offer better performance with compression support, which reduces the size of the data on the disk and consequently the I/O and CPU resources required to deserialize data.},
  address   = {Berkeley, CA},
  booktitle = {Practical Hadoop Ecosystem: A Definitive Guide to Hadoop-Related Frameworks and Tools},
  doi       = {10.1007/978-1-4842-2199-0_8},
  isbn      = {978-1-4842-2199-0},
  pages     = {325--335},
  publisher = {Apress}
}

@inproceedings{dataXFormer,
  keywords  = {datasets},
  title     = {DataXFormer: A robust transformation discovery system},
  author    = {Abedjan, Ziawasch and Morcos, John and Ilyas, Ihab F. and Ouzzani, Mourad and Papotti, Paolo and Stonebraker, Michael},
  year      = {2016},
  booktitle = {2016 IEEE 32nd International Conference on Data Engineering (ICDE)},
  doi       = {10.1109/ICDE.2016.7498319},
  number    = {},
  pages     = {1134-1145},
  volume    = {}
}


@article{gittables-article,
  keywords = {datasets},
  title    = {GitTables: A Large-Scale Corpus of Relational Tables},
  author   = {Hulsebos, Madelon and Demiralp, {\c{C}}a{\u{g}}atay and Groth, Paul},
  year     = {2021},
  url      = {https://arxiv.org/abs/2106.07258},
  journal  = {arXiv preprint arXiv:2106.07258}
}

@inproceedings{gordian,
  keywords  = {existingTechniques},
  title     = {GORDIAN: Efficient and scalable discovery of composite keys},
  author    = {Sismanis, Yannis and Brown, Paul and Haas, Peter and Reinwald, Berthold},
  year      = {2006},
  booktitle = {VLDB},
  month     = {01},
  pages     = {691-702}
}

@online{ml-visual-explanation,
  keywords = {machineLearning, basics},
  title    = {What Is Machine Learning? – A Visual Explanation},
  author   = {Markus Schmitt},
  url      = {https://www.datarevenue.com/en-blog/what-is-machine-learning-a-visual-explanation},
  urldate  = {2022-06-21}
}

@online{what-is-ml,
  keywords = {machineLearning, basics},
  title    = {Machine Learning},
  author   = {IBM Cloud Education},
  url      = {https://www.ibm.com/cloud/learn/machine-learning},
  date     = {2020-07-15},
  urldate  = {2022-05-18}
}

@online{supervised-unsupervised-learning,
  keywords = {machineLearning, classicalMachineLearning},
  title    = {Supervised vs. Unsupervised Learning: What’s the Difference?},
  author   = {Julianna Delua},
  url      = {https://www.ibm.com/cloud/blog/supervised-vs-unsupervised-learning},
  date     = {2021-03-12},
  urldate  = {2022-06-23}
}

@online{classical-ml,
  keywords = {machineLearning, classicalMachineLearning},
  title    = {Classical Machine Learning — Supervised Learning Edition},
  author   = {Bardh Rushiti},
  url      = {https://medium.com/swlh/classical-machine-learning-7efc6674fca1},
  date     = {2020-08-03},
  urldate  = {2022-06-21}
}
@online{types-of-ml,
  keywords = {machineLearning, classicalMachineLearning, neuralNet},
  title    = {Models for machine learning},
  author   = {Tim Jones},
  url      = {https://developer.ibm.com/articles/cc-models-machine-learning/},
  date     = {2017-12-04},
  urldate  = {2022-06-23}
}

@online{neuralNet,
  keywords = {machineLearning, neuralNet},
  title    = {Understanding Neural Networks},
  author   = {Tony Yiu},
  url      = {https://towardsdatascience.com/understanding-neural-networks-19020b758230},
  date     = {2019-06-02},
  urldate  = {2022-06-21}
}
@online{neuralNet-applications,
  keywords = {machineLearning, neuralNet},
  title    = {8 Applications of Neural Networks},
  author   = {Vanshika Kaushik},
  url      = {https://www.analyticssteps.com/blogs/8-applications-neural-networks},
  date     = {2021-08-27},
  urldate  = {2022-06-21}
}

@article{mlTutorial,
  keywords     = {MLTutorial},
  title        = {Machine Learning with Python: Classification (complete tutorial)},
  author       = {Mauro Di Pietro},
  year         = {2020},
  url          = {https://towardsdatascience.com/machine-learning-with-python-classification-complete-tutorial-d2c99dc524ec},
  journaltitle = {towardsdatascience.com}
}

@article{dataProfilingMetanome,
  keywords   = {starter},
  title      = {Data Profiling with Metanome},
  author     = {Papenbrock, Thorsten and Bergmann, Tanja and Finke, Moritz and Zwiener, Jakob and Naumann, Felix},
  year       = {2015},
  url        = {http://dx.doi.org/10.14778/2824032.2824086},
  acmid      = {2824086},
  doi        = {10.14778/2824032.2824086},
  issn       = {2150-8097},
  issue_date = {August 2015},
  journal    = {Proc. VLDB Endow.},
  month      = {Aug},
  number     = {12},
  numpages   = {4},
  pages      = {1860--1863},
  publisher  = {VLDB Endowment},
  volume     = {8}
}

@article{profilingRelData,
  keywords = {starter},
  title    = {Profiling relational data: a survey},
  author   = {Abedjan, Ziawasch
              and Golab, Lukasz
              and Naumann, Felix},
  year     = {2015},
  url      = {https://doi.org/10.1007/s00778-015-0389-y},
  abstract = {Profiling data to determine metadata about a given dataset is an important and frequent activity of any IT professional and researcher and is necessary for various use-cases. It encompasses a vast array of methods to examine datasets and produce metadata. Among the simpler results are statistics, such as the number of null values and distinct values in a column, its data type, or the most frequent patterns of its data values. Metadata that are more difficult to compute involve multiple columns, namely correlations, unique column combinations, functional dependencies, and inclusion dependencies. Further techniques detect conditional properties of the dataset at hand. This survey provides a classification of data profiling tasks and comprehensively reviews the state of the art for each class. In addition, we review data profiling tools and systems from research and industry. We conclude with an outlook on the future of data profiling beyond traditional profiling tasks and beyond relational databases.},
  day      = {01},
  doi      = {10.1007/s00778-015-0389-y},
  issn     = {0949-877X},
  journal  = {The VLDB Journal},
  month    = {Aug},
  number   = {4},
  pages    = {557-581},
  volume   = {24}
}

@article{ducc,
  keywords   = {starter, existingTechniques},
  title      = {Scalable Discovery of Unique Column Combinations},
  author     = {Heise, Arvid and Quian\'{e}-Ruiz, Jorge-Arnulfo and Abedjan, Ziawasch and Jentzsch, Anja and Naumann, Felix},
  year       = {2013},
  url        = {https://doi.org/10.14778/2732240.2732248},
  abstract   = {The discovery of all unique (and non-unique) column combinations in a given dataset is at the core of any data profiling effort. The results are useful for a large number of areas of data management, such as anomaly detection, data integration, data modeling, duplicate detection, indexing, and query optimization. However, discovering all unique and non-unique column combinations is an NP-hard problem, which in principle requires to verify an exponential number of column combinations for uniqueness on all data values. Thus, achieving efficiency and scalability in this context is a tremendous challenge by itself.In this paper, we devise Ducc, a scalable and efficient approach to the problem of finding all unique and non-unique column combinations in big datasets. We first model the problem as a graph coloring problem and analyze the pruning effect of individual combinations. We then present our hybrid column-based pruning technique, which traverses the lattice in a depth-first and random walk combination. This strategy allows Ducc to typically depend on the solution set size and hence to prune large swaths of the lattice. Ducc also incorporates row-based pruning to run uniqueness checks in just few milliseconds. To achieve even higher scalability, Ducc runs on several CPU cores (scale-up) and compute nodes (scale-out) with a very low overhead. We exhaustively evaluate Ducc using three datasets (two real and one synthetic) with several millions rows and hundreds of attributes. We compare Ducc with related work: Gordian and HCA. The results show that Ducc is up to more than 2 orders of magnitude faster than Gordian and HCA (631x faster than Gordian and 398x faster than HCA). Finally, a series of scalability experiments shows the efficiency of Ducc to scale up and out.},
  doi        = {10.14778/2732240.2732248},
  issn       = {2150-8097},
  issue_date = {December 2013},
  journal    = {Proc. VLDB Endow.},
  month      = {dec},
  number     = {4},
  numpages   = {12},
  pages      = {301–312},
  publisher  = {VLDB Endowment},
  volume     = {7}
}
