% LTex: enabled=false

\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
% \usepackage[left=2.5cm,right=2.5cm,top=1.5cm,bottom=1.5cm]{geometry}
\usepackage{parskip}
\usepackage{lmodern}
\usepackage{nicefrac}
\usepackage{contour}
\usepackage{ulem}
\renewcommand{\ULdepth}{1.8pt}
\contourlength{0.8pt}
\renewcommand{\underline}[1]{%
  \uline{\phantom{#1}}%
  \llap{\contour{white}{#1}}%
}
\usepackage{siunitx}
\sisetup{
    locale=DE,
    group-digits = integer,
    round-mode = places,
    round-precision = 3,
    round-pad = false
}
\usepackage{enumitem}
\usepackage{tabularx} 
\usepackage{ragged2e}
\newcolumntype{L}{>{\RaggedRight}X}
\newcolumntype{R}{>{\RaggedLeft}X}
\newcolumntype{C}{>{\centering\arraybackslash}X}
\newcommand{\hrc}[1]{\multicolumn{1}{C}{#1}}
\usepackage{multirow}
\usepackage{booktabs}
% \usepackage[dvipsnames]{xcolor, colortbl}
% \definecolor{Gray}{gray}{0.7}
% \definecolor{Lightgray}{gray}{0.9}
% \definecolor{Hellblau}{HTML}{C3F9FD}

% \setcounter{secnumdepth}{0}
% \setcounter{tocdepth}{2}

\usepackage{csquotes}
\usepackage[noorphans,font=itshape]{quoting}
\usepackage{graphicx}
\usepackage[section]{placeins}
\usepackage{listings}

\clubpenalty = 10000
\widowpenalty = 10000
\displaywidowpenalty = 10000

\usepackage{fancyhdr}
\usepackage{lastpage}
\fancypagestyle{mypagestyle}{%
  \fancyhf{}%
  % \renewcommand{\footrulewidth}{0.1mm}%
  % \fancyfoot[R]{Assignment No.}%
  % \fancyfoot[C]{\thepage}%
  % \fancyfoot[L]{\today}%
  \fancyhead[L]{}
  \fancyhead[C]{Expose}
  \fancyhead[R]{\today}
  % \renewcommand{\headrulewidth}{0mm}%
  \cfoot{Seite \thepage\ von \pageref{LastPage}}
}  
\pagestyle{mypagestyle}
\fancypagestyle{firstpagestyle}{%
  \fancyhf{}%
  % \renewcommand{\footrulewidth}{0.1mm}%
  % \fancyfoot[R]{Assignment No.}%
  % \fancyfoot[C]{\thepage}%
  % \fancyfoot[L]{\today}%
  \renewcommand{\headrulewidth}{0mm}%
  \cfoot{Seite \thepage\ von \pageref{LastPage}}
}  

\usepackage[hidelinks]{hyperref}
\hypersetup{
      pdftitle={Expose},
      pdfsubject={},
      pdfauthor={Janek Prange},
      colorlinks=false,
      pdfpagemode=UseNone
      }
      
      % Ltex: enabled=true language=en-US

\begin{document}
\thispagestyle{firstpagestyle}
\begin{center}
  \huge \textbf{Expose}\\[8pt]
  % Zusammenfassung\\[10pt]
  \normalsize Janek Prange\\
  \today
\end{center}

\section{Motivation}
Big collections of data and therefore databases are becoming increasingly important with time as more data is produced with accelerating speed. In the best case, this data is directly saved in a format that contains a lot of metadata such as data types, primary keys and information about dependencies between columns.

In many scenarios however, it is not possible to design a proper schema suited for specific data beforehand. Instead, the data is already present in an arbitrary format or is being taken from other sources such as internet dumps. These use cases require tools to -- as much as possible automatically -- detect the needed metadata from the raw data. % chktex 8

The thesis introduced in this expose tries to propose a new solution to this problem making use of machine learning.

% \section{State of the art}

\section{Naive Procedures}\label{sec:naiveProcedures}

\section{Proposed Solution}
The thesis introduced here will present a solution to find primary keys respectively primary key candidates that is primarily based on a neural network, which will be trained on a large set of tables. The hope is that the network will discover primary key candidates with better efficiency than the naive solution described in Section~\ref{sec:naiveProcedures}.

In addition to the usage of neural networks, there may be other ideas to increase the efficiency of the algorithm. One possibility would be to sample the columns of the table instead of reading all values. This way, the data that has to be retrieved from the database could be reduced which may result in a significant speedup when the algorithm is used with large datasets.

Another opportunity for increased efficiency could be the implementation of a two phase algorithm. In the first phase, columns which can't practicably be primary keys will be eliminated, such as columns containing images, videos or relatively long texts. In the second phase, the neural network would try detecting the best primary key candidates from the remaining columns.

To reduce the scope of the thesis, the focus of the algorithm will be primarily on detecting single column primary key candidates. If the results are promising and there is time left, multicolumn candidates will be included.

\section{Evaluation Method and Metric}
\begin{itemize}
  \item What is a success for the neural network?
        \begin{itemize}[label=\(\rightarrow \)]
          \item efficiency respectively speed
          \item correctness
        \end{itemize}
  \item comparison to the naive approach presented in Section~\ref{sec:naiveProcedures}
\end{itemize}

\end{document}