% LTex: enabled=false
\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}

% \usepackage[left=2.5cm,right=2.5cm,top=1.5cm,bottom=1.5cm]{geometry}
\usepackage{parskip}
\usepackage{lmodern}
\usepackage{nicefrac}

\usepackage{contour}
\usepackage{ulem}
\renewcommand{\ULdepth}{1.8pt}
\contourlength{0.8pt}
\renewcommand{\underline}[1]{%
  \uline{\phantom{#1}}%
  \llap{\contour{white}{#1}}%
}

\usepackage{siunitx}
\sisetup{
    locale=DE,
    group-digits = integer,
    round-mode = places,
    round-precision = 3,
    round-pad = false
}

\usepackage{enumitem}

\usepackage{tabularx} 
\usepackage{ragged2e}
\newcolumntype{L}{>{\RaggedRight}X}
\newcolumntype{R}{>{\RaggedLeft}X}
\newcolumntype{C}{>{\centering\arraybackslash}X}
\newcommand{\hrc}[1]{\multicolumn{1}{C}{#1}}
\usepackage{multirow}
\usepackage{booktabs}

% \usepackage[dvipsnames]{xcolor, colortbl}
% \definecolor{Gray}{gray}{0.7}
% \definecolor{Lightgray}{gray}{0.9}
% \definecolor{Hellblau}{HTML}{C3F9FD}

\usepackage{csquotes}
\usepackage[noorphans,font=itshape]{quoting}

\usepackage{graphicx}
\usepackage{float}
\usepackage[section]{placeins}
\usepackage{listings}

\usepackage{fancyhdr}
\usepackage{lastpage}
\fancypagestyle{mainpagestyle}{%
  \fancyhf{}%
  \fancyhead[L]{}%
  \fancyhead[C]{Exposé}%
  \fancyhead[R]{\today}%
  \cfoot{Page \thepage\ of \pageref{LastPage}}%
}  
\fancypagestyle{firstpagestyle}{%
  \fancyhf{}%
  \renewcommand{\headrulewidth}{0mm}%
  \cfoot{Page \thepage\ of \pageref{LastPage}}
}  
\pagestyle{mainpagestyle}

\usepackage[style=trad-plain, sorting=none, block=ragged]{biblatex}
\addbibresource{../main/literatur.bib}

% \setcounter{secnumdepth}{0}
% \setcounter{tocdepth}{2}

\clubpenalty = 10000
\widowpenalty = 10000
\displaywidowpenalty = 10000

\usepackage[hidelinks]{hyperref}
\hypersetup{
      pdftitle={Exposé},
      pdfsubject={Improving Primary Key Detection with Machine Learning},
      pdfauthor={Janek Prange},
      colorlinks=false,
      pdfpagemode=UseNone
      }
      
% Ltex: enabled=true language=en-US

\begin{document}
\thispagestyle{firstpagestyle}
\begin{center}
  \huge Improving Primary Key Detection with Machine Learning\\[8pt]
  \Large \textbf{Exposé}\\[10pt]
  \normalsize Janek Prange\\
  \today
\end{center}

\section{Motivation}
Big collections of data and therefore databases are becoming increasingly important as more data is produced with accelerating speed. In the best case, this data is directly saved in a format that contains a lot of metadata such as data types, primary keys and information about dependencies between columns.

In many scenarios however, it is not possible to design a proper schema suited for specific data beforehand. Instead, the data is already present in an arbitrary format or is being taken from other sources such as the internet, where the data format can rarely be influenced. These use cases require tools to -- as much as possible automatically -- detect the needed metadata from the raw data. % chktex 8

The thesis introduced in this exposé tries to propose a new solution to this problem making use of machine learning.


\section{Existing Techniques}\label{sec:naiveProcedures}
One of the main characteristics of a primary key is the uniqueness of its values. Based on this constraint, a naive way of finding primary key candidates would be to search for columns or column combinations which do not have any duplicate values. This could be achieved for example by sorting or hashing.

One existing algorithm which uses this characteristic is GORDIAN\footfullcite{gordian}. It works by organizing the data into a prefix tree, finding the largest column combinations that have duplicate values (maximal non-uniques) and from those compute the minimal column combinations which exclusively contain unique values (minimal uniques).

This technique has the disadvantage that the work increases with the size of the column combinations with duplicate values and is therefore not suited for large databases.

Another algorithm which tries to solve the problem of primary key detection efficiently is DUCC\footfullcite{ducc}. It has a vastly higher efficiency than GORDIAN, even though it detects both minimal uniques and maximal non-uniques at the same time. This is possible primarily because of its focus on aggressively pruning column combinations which can not be part of the desired solution.


\section{Proposed Solution}\label{sec:proposedSolution}
The thesis introduced here will try to increase the efficiency of finding primary key candidates using a neural network  which will be trained on a large set of tables. The hope is that the network will discover primary key candidates with better efficiency than the solutions described in Section~\ref{sec:naiveProcedures}.

In addition to the usage of neural networks, there may be other ideas to increase the efficiency of the algorithm. One possibility would be to sample from the columns of the table instead of reading all values. This way, the data that has to be retrieved from the database could be reduced, which may result in a significant speedup when the algorithm is used with large datasets.

Another opportunity for increased efficiency could be to narrow the selection of possible primary key candidates with filters. To begin with, columns which can not practicably be primary keys will be eliminated, such as columns containing images, videos or long texts. As a later filter, the neural network would try to detect the best primary key candidates from the remaining columns.

To reduce the scope of the thesis, the focus of the algorithm will be primarily on detecting single column primary key candidates. If the results are promising and there is time left, multicolumn candidates will be included.


\section{Evaluation Method and Metric}
To train the neural network that is at the center of the solution proposed in Section~\ref{sec:proposedSolution}, methods have to be defined to evaluate the success or failure of the network. In this case, there are two favorable and relatively easy to measure metrics, which are the correctness and the efficiency of the algorithm.

A primary key candidate is correct when it has no duplicate elements and no subset of it is a column combination only consisting of unique values. These requirements are easy to check, especially if the algorithm is limited to single column candidates.

For the purposes of this thesis, the algorithm is efficient if it is faster and uses less main memory than the naive solution presented in Section~\ref{sec:naiveProcedures}, which will be tested on the same data as the neural network.

\end{document}